{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_necessary_lib():\n",
    "    # Preload NLTK data\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # Load the dataset if not present\n",
    "    try:\n",
    "        if os.path.exists('suicide_dataset.csv'):\n",
    "            print(\"File exists\")\n",
    "            dataset_path = 'suicide_dataset.csv'\n",
    "        else:\n",
    "            dataset = load_dataset(\"Ram07/Detection-for-Suicide\")\n",
    "            df = pd.DataFrame(dataset['train'])\n",
    "            df.to_csv('suicide_dataset.csv', index=False)\n",
    "            dataset_path = 'suicide_dataset.csv'\n",
    "    except Exception as e:\n",
    "        print(\"Error loading dataset:\", e)\n",
    "\n",
    "    finally:\n",
    "        # Load data directly from the CSV file\n",
    "        try:\n",
    "            data = pd.read_csv(dataset_path)\n",
    "            print(\"Database loaded ...._\")\n",
    "            return clean_NA_data(data)\n",
    "        except Exception as e:\n",
    "            print(\"Error loading dataset:\", e)\n",
    "            \n",
    "            sys.exit()\n",
    "\n",
    "# //Remove rows without \"suicidal\" or \"non-suicidal\" labels\n",
    "# //data = data[data['class'].isin(['suicidal', 'non-suicidal'])]\n",
    "# !-----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n"
     ]
    }
   ],
   "source": [
    "def clean_NA_data(X):\n",
    "    print(X.isna().sum())\n",
    "    X.fillna(X.mean(), inplace=True)\n",
    "    return X\n",
    "    \n",
    "\n",
    "\n",
    "load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # print(\"preprocessing starting ...........\")\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_SDM(X_train,y_train):\n",
    "    # Train a classification model (SVM)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    classifier = SVC(kernel='linear', verbose=True, probability=True)  # Enable probability estimates\n",
    "    model = make_pipeline(vectorizer, classifier)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model trained successfully.\")\n",
    "    save_model()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the model as a .pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():     \n",
    "    # Save the trained model\n",
    "    try:\n",
    "        joblib.dump(model, 'suicide_detection_model.pkl')\n",
    "        print(\"Model saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error saving model:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_SDM(model,X_test, y_test):\n",
    "    # Evaluate the model on the test set\n",
    "    try:\n",
    "        accuracy = model.score(X_test, y_test)\n",
    "        print(\"Model Accuracy on Test Set:\", round(accuracy*100,3),\"%\")\n",
    "    except Exception as e:\n",
    "        print(\"Error evaluating model:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_model(X_train,y_train):\n",
    "    print('start_model'.center(30,\"_\"))\n",
    "    if os.path.exists(\"suicide_detection_model.pkl\"):    \n",
    "        # Load the saved model\n",
    "        try:\n",
    "            model = joblib.load('suicide_detection_model.pkl')\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(\"Error loading the model:\", e)\n",
    "    else:\n",
    "        model=train_model_SDM(X_train,y_train)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_progress(text_data, save_file=None):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess_text(text):\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "        return ' '.join(filtered_tokens)\n",
    "\n",
    "    if save_file and os.path.exists(save_file):\n",
    "        # If file exists, load processed text from file\n",
    "        processed_text = pd.read_csv(save_file, header=None)[0].tolist()\n",
    "    else:\n",
    "        tqdm.pandas()\n",
    "        processed_text = text_data.progress_apply(preprocess_text)\n",
    "        \n",
    "        # Save processed text to a file\n",
    "        if save_file:\n",
    "            processed_text.to_csv(save_file, index=False, header=False)\n",
    "\n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the detection program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    load_necessary_lib()\n",
    "    data=load_dataset()\n",
    "    # print(data)\n",
    "\n",
    "    # Preprocess text\n",
    "\n",
    "    # data['text'] = data['text'].apply(preprocess_text)\n",
    "    data['text'] = preprocess_text_with_progress(data['text'],save_file='final_cleaned_processed_text.csv')   # ?with loading animation\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['text'], data['class'], test_size=0.2, random_state=42)\n",
    "    print(\"Spliting Done\")\n",
    "    model=start_model(X_train,y_train)\n",
    "    evaluating_SDM(model,X_test, y_test)\n",
    "    \n",
    "\n",
    "    cont=True\n",
    "    while cont==True:\n",
    "        # -------------------------->>>>>>>  Accept user input\n",
    "        try:\n",
    "            print(\"**\".center(120,\"-\"),\"\\n\\n\")\n",
    "            user_input = input(\"Enter your response: \").strip()\n",
    "            if user_input==\"quit\":\n",
    "                cont=False\n",
    "            elif user_input:\n",
    "                # Preprocess user input and predict\n",
    "                preprocessed_input = preprocess_text(user_input)\n",
    "                prediction_scores = model.predict_proba([preprocessed_input])[0]\n",
    "                prediction = model.predict([preprocessed_input])[0]\n",
    "\n",
    "                # Convert prediction back to original labels\n",
    "                predicted_label = 'suicidal' if prediction == 0 else 'non-suicidal'  # Adjusted this line\n",
    "                # suicidal_p_scores[1]=prediction_scores[1] * 100\n",
    "                # non_suicidal_p_scores[0]=predicted_scores[0] * 100\n",
    "                # Output prediction result and scores\n",
    "                print(\"Prediction Score for Suicidal:\", \"{:.2f}%\".format(prediction_scores[0] * 100))\n",
    "                print(\"Prediction Score for Non-Suicidal:\", \"{:.2f}%\".format(prediction_scores[1] * 100))\n",
    "                \n",
    "                return (prediction_scores,model,X_train, X_test, y_train, y_test)\n",
    "            else:\n",
    "                print(\"Empty input. Please provide a response.\")\n",
    "        except Exception as e:\n",
    "            print(\"Error processing user input because ;\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n",
      "File exists\n",
      "Database loaded ...._\n",
      "Spliting Done\n",
      "_________start_model__________\n",
      "Error evaluating model: np.nan is an invalid document, expected byte or unicode string.\n",
      "-----------------------------------------------------------**----------------------------------------------------------- \n",
      "\n",
      "\n",
      "Prediction Score for Suicidal: 0.00%\n",
      "Prediction Score for Non-Suicidal: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    prediction_scores,model,X_train, X_test, y_train, y_test=main()\n",
    "    # plot_confusion_matrix(model, X_test, y_test)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nExiting the program.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from seaborn) (2.2.1)\n",
      "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
      "  Using cached matplotlib-3.8.3-cp312-cp312-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached contourpy-1.2.0-cp312-cp312-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached fonttools-4.50.0-cp312-cp312-win_amd64.whl.metadata (162 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached kiwisolver-1.4.5-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
      "Collecting pillow>=8 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached pillow-10.2.0-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached matplotlib-3.8.3-cp312-cp312-win_amd64.whl (7.6 MB)\n",
      "Using cached contourpy-1.2.0-cp312-cp312-win_amd64.whl (187 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.50.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.5-cp312-cp312-win_amd64.whl (56 kB)\n",
      "Using cached pillow-10.2.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.50.0 kiwisolver-1.4.5 matplotlib-3.8.3 pillow-10.2.0 pyparsing-3.1.2 seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\lucifer-drive\\programs\\suicide-rate-prediction-with-machine-learning-master\\suicide-rate-prediction-with-machine-learning-master\\ev_ker\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'model' contains the trained model and 'X_test' contains the test data\n",
    "# Make predictions on the test data\n",
    "def plot_confusion_matrix(model, X_test, y_test):    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt='g', xticklabels=['Non-Suicidal', 'Suicidal'], yticklabels=['Non-Suicidal', 'Suicidal'])\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m, in \u001b[0;36mplot_confusion_matrix\u001b[1;34m(model, X_test, y_test)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_confusion_matrix\u001b[39m(model, X_test, y_test):    \n\u001b[1;32m----> 4\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Generate confusion matrix\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     conf_matrix \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_pred)\n",
      "File \u001b[1;32md:\\Lucifer-Drive\\Programs\\Suicide-Rate-Prediction-with-Machine-Learning-master\\Suicide-Rate-Prediction-with-Machine-Learning-master\\ev_ker\\Lib\\site-packages\\sklearn\\pipeline.py:602\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 602\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32md:\\Lucifer-Drive\\Programs\\Suicide-Rate-Prediction-with-Machine-Learning-master\\Suicide-Rate-Prediction-with-Machine-Learning-master\\ev_ker\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2162\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2146\u001b[0m \n\u001b[0;32m   2147\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2158\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2160\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2162\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Lucifer-Drive\\Programs\\Suicide-Rate-Prediction-with-Machine-Learning-master\\Suicide-Rate-Prediction-with-Machine-Learning-master\\ev_ker\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1434\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1434\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1436\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Lucifer-Drive\\Programs\\Suicide-Rate-Prediction-with-Machine-Learning-master\\Suicide-Rate-Prediction-with-Machine-Learning-master\\ev_ker\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32md:\\Lucifer-Drive\\Programs\\Suicide-Rate-Prediction-with-Machine-Learning-master\\Suicide-Rate-Prediction-with-Machine-Learning-master\\ev_ker\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:105\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32md:\\Lucifer-Drive\\Programs\\Suicide-Rate-Prediction-with-Machine-Learning-master\\Suicide-Rate-Prediction-with-Machine-Learning-master\\ev_ker\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    235\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ev_ker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
